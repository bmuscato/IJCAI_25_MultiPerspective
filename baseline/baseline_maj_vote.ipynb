{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/home/bmuscato/venv_b/share/doc/networkx-3.1/dataset/code_TACL/train_sum_soft.csv\")\n",
    "test=pd.read_csv(\"/home/bmuscato/venv_b/share/doc/networkx-3.1/dataset/code_TACL/test_sum_soft.csv\")\n",
    "val=pd.read_csv(\"/home/bmuscato/venv_b/share/doc/networkx-3.1/dataset/code_TACL/val_sum_soft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./multiclassification_stance/{model_name_filename}/results/human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>QID_x</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Query</th>\n",
       "      <th>docID</th>\n",
       "      <th>docURL</th>\n",
       "      <th>docTitle</th>\n",
       "      <th>docCont</th>\n",
       "      <th>engineID</th>\n",
       "      <th>...</th>\n",
       "      <th>majority_label</th>\n",
       "      <th>labels</th>\n",
       "      <th>Input</th>\n",
       "      <th>Input_length</th>\n",
       "      <th>docCont_length</th>\n",
       "      <th>gpt_summaries</th>\n",
       "      <th>doc</th>\n",
       "      <th>label</th>\n",
       "      <th>label_indices</th>\n",
       "      <th>soft_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>Fighting in Hockey</td>\n",
       "      <td>Should Fighting Be Allowed in Hockey?</td>\n",
       "      <td>2s50q1r</td>\n",
       "      <td>https://www.nbcsports.com/washington/capitals/...</td>\n",
       "      <td>Explaining the unwritten rules and etiquette o...</td>\n",
       "      <td>Lars Eller saw his target. He quickly skated o...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Pro</td>\n",
       "      <td>['Pro', 'Pro', 'Pro']</td>\n",
       "      <td>Should Fighting Be Allowed in Hockey? Explaini...</td>\n",
       "      <td>2972</td>\n",
       "      <td>2952</td>\n",
       "      <td>In a heated NHL matchup between the Washington...</td>\n",
       "      <td>In a heated NHL matchup between the Washington...</td>\n",
       "      <td>['Pro', 'Pro', 'Pro']</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>Prostitution - Legalize It</td>\n",
       "      <td>Should Prostitution Be Legal?</td>\n",
       "      <td>1s42q3r</td>\n",
       "      <td>https://medium.com/@nkuphirun/a-global-debate-...</td>\n",
       "      <td>A Global debate: Should Prostitution be Legali...</td>\n",
       "      <td>Human Trafficking, one of the world‚Äôs greatest...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['Neutral', 'Neutral', 'Neutral']</td>\n",
       "      <td>Should Prostitution Be Legal? A Global debate:...</td>\n",
       "      <td>1483</td>\n",
       "      <td>1467</td>\n",
       "      <td>The article presents a comprehensive discussio...</td>\n",
       "      <td>The article presents a comprehensive discussio...</td>\n",
       "      <td>['Neutral', 'Neutral', 'Neutral']</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>Drone Strikes Overseas</td>\n",
       "      <td>Should the United States Continue Its Use of D...</td>\n",
       "      <td>2s40q10r</td>\n",
       "      <td>https://www.popmatters.com/iron-man-3-shane-bl...</td>\n",
       "      <td>'Iron Man 3' Finds Its Hero in Crisis - PopMat...</td>\n",
       "      <td>Marvel Studios launched its second phase of fi...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Not-about</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>Should the United States Continue Its Use of D...</td>\n",
       "      <td>5683</td>\n",
       "      <td>5662</td>\n",
       "      <td>Marvel Studios‚Äô ‚ÄúIron Man 3,‚Äù directed by Shan...</td>\n",
       "      <td>Marvel Studios‚Äô ‚ÄúIron Man 3,‚Äù directed by Shan...</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Prescription Drug Ads</td>\n",
       "      <td>Should Prescription Drugs Be Advertised Direct...</td>\n",
       "      <td>2s11q4r</td>\n",
       "      <td>https://www.techtimes.com/articles/238486/2019...</td>\n",
       "      <td>J&amp;J First Big Pharma Company To Add Prices To ...</td>\n",
       "      <td>This is a modal window.Johnson &amp; Johnson has a...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['Neutral', 'Neutral', 'Pro']</td>\n",
       "      <td>Should Prescription Drugs Be Advertised Direct...</td>\n",
       "      <td>400</td>\n",
       "      <td>377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is a modal window.Johnson &amp; Johnson has a...</td>\n",
       "      <td>['Neutral', 'Neutral', 'Pro']</td>\n",
       "      <td>[2, 2, 0]</td>\n",
       "      <td>[0.2, 0.1, 0.6, 0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>Ronald Reagan</td>\n",
       "      <td>Was Ronald Reagan a Good President?</td>\n",
       "      <td>2s33q1r</td>\n",
       "      <td>https://71republic.com/2019/02/16/ronald-reaga...</td>\n",
       "      <td>Just How Good Was Ronald Reagan As President?</td>\n",
       "      <td>In the modern-day Republican Party, Ronald Rea...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['Neutral', 'Neutral', 'Neutral']</td>\n",
       "      <td>Was Ronald Reagan a Good President? Just How G...</td>\n",
       "      <td>1029</td>\n",
       "      <td>1015</td>\n",
       "      <td>Ronald Reagan, a revered figure in the Republi...</td>\n",
       "      <td>Ronald Reagan, a revered figure in the Republi...</td>\n",
       "      <td>['Neutral', 'Neutral', 'Neutral']</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>711</td>\n",
       "      <td>729</td>\n",
       "      <td>44</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>Are Social Networking Sites Good for Our Society?</td>\n",
       "      <td>2s44q2r</td>\n",
       "      <td>https://www.zmescience.com/science/native-amer...</td>\n",
       "      <td>Native American societies had their own brand ...</td>\n",
       "      <td>Cookie NoticeWe use cookies to personalize con...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No majority</td>\n",
       "      <td>['Pro', 'Neutral', 'Not-about']</td>\n",
       "      <td>Are Social Networking Sites Good for Our Socie...</td>\n",
       "      <td>931</td>\n",
       "      <td>913</td>\n",
       "      <td>The study conducted by Jacob Lulewicz, a lectu...</td>\n",
       "      <td>The study conducted by Jacob Lulewicz, a lectu...</td>\n",
       "      <td>['Pro', 'Neutral', 'Not-about']</td>\n",
       "      <td>[0, 2, 3]</td>\n",
       "      <td>[0.3, 0.1, 0.3, 0.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>712</td>\n",
       "      <td>730</td>\n",
       "      <td>42</td>\n",
       "      <td>Prostitution - Legalize It</td>\n",
       "      <td>Should Prostitution Be Legal?</td>\n",
       "      <td>2s42q9r</td>\n",
       "      <td>https://helenair.com/opinion/letters/this-week...</td>\n",
       "      <td>This week's letters to the editor: Border wall...</td>\n",
       "      <td>¬© 2019 Lee EnterprisesTerms of Service  |  Pri...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Not-about</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>Should Prostitution Be Legal? This week's lett...</td>\n",
       "      <td>2647</td>\n",
       "      <td>2628</td>\n",
       "      <td>The Helena Independent Record recently feature...</td>\n",
       "      <td>The Helena Independent Record recently feature...</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>713</td>\n",
       "      <td>731</td>\n",
       "      <td>13</td>\n",
       "      <td>Standardized Tests</td>\n",
       "      <td>Is the Use of Standardized Tests Improving Edu...</td>\n",
       "      <td>1s13q8r</td>\n",
       "      <td>https://thinkprogress.org/why-we-need-to-impro...</td>\n",
       "      <td>Why We Need To Improve Standardized Testing ‚Äì ...</td>\n",
       "      <td>Parents who oppose standardized testing have b...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>['Pro', 'Neutral', 'Neutral']</td>\n",
       "      <td>Is the Use of Standardized Tests Improving Edu...</td>\n",
       "      <td>770</td>\n",
       "      <td>751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parents who oppose standardized testing have b...</td>\n",
       "      <td>['Pro', 'Neutral', 'Neutral']</td>\n",
       "      <td>[0, 2, 2]</td>\n",
       "      <td>[0.2, 0.1, 0.6, 0.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>714</td>\n",
       "      <td>732</td>\n",
       "      <td>23</td>\n",
       "      <td>Concealed Handguns</td>\n",
       "      <td>Should Adults Have the Right to Carry a Concea...</td>\n",
       "      <td>2s23q5r</td>\n",
       "      <td>https://www.foxnews.com/us/new-study-finds-gro...</td>\n",
       "      <td>New study finds growing demand for concealed h...</td>\n",
       "      <td>This material may not be published, broadcast,...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No majority</td>\n",
       "      <td>['Not-about', 'Pro', 'Neutral']</td>\n",
       "      <td>Should Adults Have the Right to Carry a Concea...</td>\n",
       "      <td>547</td>\n",
       "      <td>525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This material may not be published, broadcast,...</td>\n",
       "      <td>['Not-about', 'Pro', 'Neutral']</td>\n",
       "      <td>[3, 0, 2]</td>\n",
       "      <td>[0.3, 0.1, 0.3, 0.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>715</td>\n",
       "      <td>733</td>\n",
       "      <td>16</td>\n",
       "      <td>Teacher Tenure</td>\n",
       "      <td>Should Teachers Get Tenure?</td>\n",
       "      <td>2s16q6r</td>\n",
       "      <td>https://www.nytimes.com/2019/02/18/us/edray-go...</td>\n",
       "      <td>For a Black Mathematician, What It‚Äôs Like to B...</td>\n",
       "      <td>Fewer than 1 percent of doctorates in math are...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Not-about</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>Should Teachers Get Tenure? For a Black Mathem...</td>\n",
       "      <td>2038</td>\n",
       "      <td>2017</td>\n",
       "      <td>Edray Goins, an accomplished African-American ...</td>\n",
       "      <td>Edray Goins, an accomplished African-American ...</td>\n",
       "      <td>['Not-about', 'Not-about', 'Not-about']</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  QID_x                       Topic  \\\n",
       "0               0           0     50          Fighting in Hockey   \n",
       "1               1           1     42  Prostitution - Legalize It   \n",
       "2               2           2     40      Drone Strikes Overseas   \n",
       "3               3           3     11       Prescription Drug Ads   \n",
       "4               4           4     33               Ronald Reagan   \n",
       "..            ...         ...    ...                         ...   \n",
       "711           711         729     44                Social Media   \n",
       "712           712         730     42  Prostitution - Legalize It   \n",
       "713           713         731     13          Standardized Tests   \n",
       "714           714         732     23          Concealed Handguns   \n",
       "715           715         733     16              Teacher Tenure   \n",
       "\n",
       "                                                 Query     docID  \\\n",
       "0                Should Fighting Be Allowed in Hockey?   2s50q1r   \n",
       "1                        Should Prostitution Be Legal?   1s42q3r   \n",
       "2    Should the United States Continue Its Use of D...  2s40q10r   \n",
       "3    Should Prescription Drugs Be Advertised Direct...   2s11q4r   \n",
       "4                  Was Ronald Reagan a Good President?   2s33q1r   \n",
       "..                                                 ...       ...   \n",
       "711  Are Social Networking Sites Good for Our Society?   2s44q2r   \n",
       "712                      Should Prostitution Be Legal?   2s42q9r   \n",
       "713  Is the Use of Standardized Tests Improving Edu...   1s13q8r   \n",
       "714  Should Adults Have the Right to Carry a Concea...   2s23q5r   \n",
       "715                        Should Teachers Get Tenure?   2s16q6r   \n",
       "\n",
       "                                                docURL  \\\n",
       "0    https://www.nbcsports.com/washington/capitals/...   \n",
       "1    https://medium.com/@nkuphirun/a-global-debate-...   \n",
       "2    https://www.popmatters.com/iron-man-3-shane-bl...   \n",
       "3    https://www.techtimes.com/articles/238486/2019...   \n",
       "4    https://71republic.com/2019/02/16/ronald-reaga...   \n",
       "..                                                 ...   \n",
       "711  https://www.zmescience.com/science/native-amer...   \n",
       "712  https://helenair.com/opinion/letters/this-week...   \n",
       "713  https://thinkprogress.org/why-we-need-to-impro...   \n",
       "714  https://www.foxnews.com/us/new-study-finds-gro...   \n",
       "715  https://www.nytimes.com/2019/02/18/us/edray-go...   \n",
       "\n",
       "                                              docTitle  \\\n",
       "0    Explaining the unwritten rules and etiquette o...   \n",
       "1    A Global debate: Should Prostitution be Legali...   \n",
       "2    'Iron Man 3' Finds Its Hero in Crisis - PopMat...   \n",
       "3    J&J First Big Pharma Company To Add Prices To ...   \n",
       "4        Just How Good Was Ronald Reagan As President?   \n",
       "..                                                 ...   \n",
       "711  Native American societies had their own brand ...   \n",
       "712  This week's letters to the editor: Border wall...   \n",
       "713  Why We Need To Improve Standardized Testing ‚Äì ...   \n",
       "714  New study finds growing demand for concealed h...   \n",
       "715  For a Black Mathematician, What It‚Äôs Like to B...   \n",
       "\n",
       "                                               docCont  engineID  ...  \\\n",
       "0    Lars Eller saw his target. He quickly skated o...         2  ...   \n",
       "1    Human Trafficking, one of the world‚Äôs greatest...         1  ...   \n",
       "2    Marvel Studios launched its second phase of fi...         2  ...   \n",
       "3    This is a modal window.Johnson & Johnson has a...         2  ...   \n",
       "4    In the modern-day Republican Party, Ronald Rea...         2  ...   \n",
       "..                                                 ...       ...  ...   \n",
       "711  Cookie NoticeWe use cookies to personalize con...         2  ...   \n",
       "712  ¬© 2019 Lee EnterprisesTerms of Service  |  Pri...         2  ...   \n",
       "713  Parents who oppose standardized testing have b...         1  ...   \n",
       "714  This material may not be published, broadcast,...         2  ...   \n",
       "715  Fewer than 1 percent of doctorates in math are...         2  ...   \n",
       "\n",
       "     majority_label                                   labels  \\\n",
       "0               Pro                    ['Pro', 'Pro', 'Pro']   \n",
       "1           Neutral        ['Neutral', 'Neutral', 'Neutral']   \n",
       "2         Not-about  ['Not-about', 'Not-about', 'Not-about']   \n",
       "3           Neutral            ['Neutral', 'Neutral', 'Pro']   \n",
       "4           Neutral        ['Neutral', 'Neutral', 'Neutral']   \n",
       "..              ...                                      ...   \n",
       "711     No majority          ['Pro', 'Neutral', 'Not-about']   \n",
       "712       Not-about  ['Not-about', 'Not-about', 'Not-about']   \n",
       "713         Neutral            ['Pro', 'Neutral', 'Neutral']   \n",
       "714     No majority          ['Not-about', 'Pro', 'Neutral']   \n",
       "715       Not-about  ['Not-about', 'Not-about', 'Not-about']   \n",
       "\n",
       "                                                 Input Input_length  \\\n",
       "0    Should Fighting Be Allowed in Hockey? Explaini...         2972   \n",
       "1    Should Prostitution Be Legal? A Global debate:...         1483   \n",
       "2    Should the United States Continue Its Use of D...         5683   \n",
       "3    Should Prescription Drugs Be Advertised Direct...          400   \n",
       "4    Was Ronald Reagan a Good President? Just How G...         1029   \n",
       "..                                                 ...          ...   \n",
       "711  Are Social Networking Sites Good for Our Socie...          931   \n",
       "712  Should Prostitution Be Legal? This week's lett...         2647   \n",
       "713  Is the Use of Standardized Tests Improving Edu...          770   \n",
       "714  Should Adults Have the Right to Carry a Concea...          547   \n",
       "715  Should Teachers Get Tenure? For a Black Mathem...         2038   \n",
       "\n",
       "     docCont_length                                      gpt_summaries  \\\n",
       "0              2952  In a heated NHL matchup between the Washington...   \n",
       "1              1467  The article presents a comprehensive discussio...   \n",
       "2              5662  Marvel Studios‚Äô ‚ÄúIron Man 3,‚Äù directed by Shan...   \n",
       "3               377                                                NaN   \n",
       "4              1015  Ronald Reagan, a revered figure in the Republi...   \n",
       "..              ...                                                ...   \n",
       "711             913  The study conducted by Jacob Lulewicz, a lectu...   \n",
       "712            2628  The Helena Independent Record recently feature...   \n",
       "713             751                                                NaN   \n",
       "714             525                                                NaN   \n",
       "715            2017  Edray Goins, an accomplished African-American ...   \n",
       "\n",
       "                                                   doc  \\\n",
       "0    In a heated NHL matchup between the Washington...   \n",
       "1    The article presents a comprehensive discussio...   \n",
       "2    Marvel Studios‚Äô ‚ÄúIron Man 3,‚Äù directed by Shan...   \n",
       "3    This is a modal window.Johnson & Johnson has a...   \n",
       "4    Ronald Reagan, a revered figure in the Republi...   \n",
       "..                                                 ...   \n",
       "711  The study conducted by Jacob Lulewicz, a lectu...   \n",
       "712  The Helena Independent Record recently feature...   \n",
       "713  Parents who oppose standardized testing have b...   \n",
       "714  This material may not be published, broadcast,...   \n",
       "715  Edray Goins, an accomplished African-American ...   \n",
       "\n",
       "                                       label  label_indices  \\\n",
       "0                      ['Pro', 'Pro', 'Pro']      [0, 0, 0]   \n",
       "1          ['Neutral', 'Neutral', 'Neutral']      [2, 2, 2]   \n",
       "2    ['Not-about', 'Not-about', 'Not-about']      [3, 3, 3]   \n",
       "3              ['Neutral', 'Neutral', 'Pro']      [2, 2, 0]   \n",
       "4          ['Neutral', 'Neutral', 'Neutral']      [2, 2, 2]   \n",
       "..                                       ...            ...   \n",
       "711          ['Pro', 'Neutral', 'Not-about']      [0, 2, 3]   \n",
       "712  ['Not-about', 'Not-about', 'Not-about']      [3, 3, 3]   \n",
       "713            ['Pro', 'Neutral', 'Neutral']      [0, 2, 2]   \n",
       "714          ['Not-about', 'Pro', 'Neutral']      [3, 0, 2]   \n",
       "715  ['Not-about', 'Not-about', 'Not-about']      [3, 3, 3]   \n",
       "\n",
       "              soft_labels  \n",
       "0    [1.0, 0.0, 0.0, 0.0]  \n",
       "1    [0.0, 0.0, 1.0, 0.0]  \n",
       "2    [0.0, 0.0, 0.0, 1.0]  \n",
       "3    [0.2, 0.1, 0.6, 0.1]  \n",
       "4    [0.0, 0.0, 1.0, 0.0]  \n",
       "..                    ...  \n",
       "711  [0.3, 0.1, 0.3, 0.3]  \n",
       "712  [0.0, 0.0, 0.0, 1.0]  \n",
       "713  [0.2, 0.1, 0.6, 0.1]  \n",
       "714  [0.3, 0.1, 0.3, 0.3]  \n",
       "715  [0.0, 0.0, 0.0, 1.0]  \n",
       "\n",
       "[716 rows x 25 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1972306/3627685613.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['labels'] = train['majority_label'].map(label_encoding)\n",
      "/tmp/ipykernel_1972306/3627685613.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['labels'] = train['labels'].astype(int)\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 619/619 [00:02<00:00, 307.60 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:00<00:00, 235.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139/139 [00:00<00:00, 228.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def no_maj(df):\n",
    "    return df.loc[df['majority_label'] != 'No majority']\n",
    "train, test, val = map(no_maj, [train, test, val]) \n",
    "\n",
    "\n",
    "label_encoding = {'Pro': 0,\n",
    "'Against': 1,\n",
    "'Neutral': 2,\n",
    "'Not-about': 3}\n",
    "\n",
    "train['labels'] = train['majority_label'].map(label_encoding)\n",
    "val['labels'] = val['majority_label'].map(label_encoding)\n",
    "test['labels'] = test['majority_label'].map(label_encoding)\n",
    "# Convert 'hard_label' column to integers to ensure consistent types\n",
    "train['labels'] = train['labels'].astype(int)\n",
    "test['labels'] = test['labels'].astype(int)\n",
    "val['labels'] = val['labels'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset objects\n",
    "train_ = Dataset.from_pandas(train, preserve_index=False)\n",
    "test_ = Dataset.from_pandas(test, preserve_index=False)\n",
    "val_ = Dataset.from_pandas(val, preserve_index=False)\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "dataset = DatasetDict({'train': train_, 'test': test_, 'val': val_})\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_func(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['Input'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    # Add labels to the tokenized inputs\n",
    "    tokenized_inputs['label'] = examples['labels']\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_tokenized = train_.map(tokenize_func, batched=True)\n",
    "val_tokenized = val_.map(tokenize_func, batched=True)\n",
    "test_tokenized = test_.map(tokenize_func, batched=True)\n",
    "\n",
    "# Convert datasets to PyTorch format\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy() \n",
    "    cross_entropy = -np.sum(np.eye(probs.shape[1])[labels] * np.log(probs + 1e-9)) / len(labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'cross_entropy': cross_entropy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1972306/2340046930.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=train_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 08:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Cross Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.398600</td>\n",
       "      <td>1.375316</td>\n",
       "      <td>0.326333</td>\n",
       "      <td>0.191780</td>\n",
       "      <td>1.375168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.392600</td>\n",
       "      <td>1.359083</td>\n",
       "      <td>0.323102</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>1.358930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.367800</td>\n",
       "      <td>1.336468</td>\n",
       "      <td>0.326333</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>1.336408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.271400</td>\n",
       "      <td>1.227643</td>\n",
       "      <td>0.418417</td>\n",
       "      <td>0.310151</td>\n",
       "      <td>1.227527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>1.045464</td>\n",
       "      <td>0.554120</td>\n",
       "      <td>0.556464</td>\n",
       "      <td>1.045691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.938900</td>\n",
       "      <td>0.795295</td>\n",
       "      <td>0.681745</td>\n",
       "      <td>0.674902</td>\n",
       "      <td>0.795458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.742300</td>\n",
       "      <td>0.610802</td>\n",
       "      <td>0.772213</td>\n",
       "      <td>0.770281</td>\n",
       "      <td>0.611183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.432226</td>\n",
       "      <td>0.848142</td>\n",
       "      <td>0.847854</td>\n",
       "      <td>0.432219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.1277449488639832, metrics={'train_runtime': 507.9958, 'train_samples_per_second': 9.748, 'train_steps_per_second': 0.315, 'total_flos': 4614955296915456.0, 'train_loss': 1.1277449488639832, 'epoch': 8.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmuscato/venv_b/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4322259724140167, 'eval_accuracy': 0.8481421647819063, 'eval_f1': 0.8478538324420677, 'eval_cross_entropy': 0.4322192390089755, 'eval_runtime': 14.4335, 'eval_samples_per_second': 42.886, 'eval_steps_per_second': 1.386, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = f'{output_dir}/best_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [00:46<00:00, 30.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bmuscato/baseline_stance/commit/313caaa6e9ed5a2345cb57400bd12db9621b4ff6', commit_message='Upload tokenizer', commit_description='', oid='313caaa6e9ed5a2345cb57400bd12db9621b4ff6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/bmuscato/baseline_stance', endpoint='https://huggingface.co', repo_type='model', repo_id='bmuscato/baseline_stance'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"bmuscato/baseline_stance\")\n",
    "tokenizer.push_to_hub(\"bmuscato/baseline_stance\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidences(df, model, tokenizer):\n",
    "    confidences = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    for text in df['Input']:\n",
    "        # Tokenize and preprocess the text input\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "            confidences.append(probabilities.max().item())  # Max confidence for the predicted class\n",
    "    \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation = True, padding = 'max_length', max_length = 512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).tolist()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        return probabilities, predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prob = []\n",
    "softmax_pred = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    text = row['Input']\n",
    "    probs, preds = predictions(text)\n",
    "    softmax_prob.append(probs)\n",
    "    softmax_pred.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['softmax_prob'] = softmax_prob\n",
    "test['softmax_preds'] = softmax_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.76258992805755\n",
      "Precision: 56.05273752012883\n",
      "Recall: 44.997995188452286\n",
      "F1 Score: 45.614236681480556\n",
      "Confusion Matrix:\n",
      "[[19  0 20  4]\n",
      " [ 0  6 22  1]\n",
      " [ 6  3 28  6]\n",
      " [ 2  0 10 12]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.44      0.54        43\n",
      "           1       0.67      0.21      0.32        29\n",
      "           2       0.35      0.65      0.46        43\n",
      "           3       0.52      0.50      0.51        24\n",
      "\n",
      "    accuracy                           0.47       139\n",
      "   macro avg       0.56      0.45      0.46       139\n",
      "weighted avg       0.56      0.47      0.46       139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = test['labels'] \n",
    "y_pred = test['softmax_preds']\n",
    "\n",
    "y_true = [int(label) for label in y_true]  \n",
    "y_pred = [int(label) for label in y_pred]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Precision:\", precision*100)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"Recall:\", recall*100)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred, average='macro')  # 'macro' averaging for multiclass\n",
    "print(\"F1 Score:\", f1*100)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidences(df, model, tokenizer):\n",
    "    confidences = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    for text in df['Input']:\n",
    "        # Tokenize and preprocess the text input\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "            confidences.append(probabilities.max().item())  # Max confidence for the predicted class\n",
    "    \n",
    "    df['confidence_scores'] = confidences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Confidence Score: 70.48201290823573\n",
      "Average Confidence for Correct Predictions: 73.23887265645541\n",
      "Average Confidence for Incorrect Predictions: 68.06044691317791\n"
     ]
    }
   ],
   "source": [
    "test_df = calculate_confidences(test, model, tokenizer)\n",
    "confidence_scores = test['confidence_scores']\n",
    "\n",
    "    # Confidence for Correct and Incorrect Predictions\n",
    "correct_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred == true])\n",
    "incorrect_confidence = np.mean([confidence for pred, true, confidence in zip(y_pred, y_true, confidence_scores) if pred != true])\n",
    "\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "\n",
    "print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"/home/bmuscato/venv_b/share/doc/networkx-3.1/dataset/code_TACL/results_roberta_baseline_f1m_stance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
