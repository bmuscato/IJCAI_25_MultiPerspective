{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from collections import Counter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import shutil\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import ast\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/train_df_preprocessed_EPIC-2.csv\")\n",
    "test=pd.read_csv(\"/test_df_preprocessed_EPIC-2.csv\")\n",
    "val=pd.read_csv(\"/val_df_preprocessed_EPIC-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'\n",
    "model_name_filename = model_name.replace(\"/\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of annotators: 8\n",
      "Maximum number of annotators: 7\n",
      "Maximum number of annotators: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_annotators(df, exclude_annotator=None):\n",
    "    \"\"\"\n",
    "    Process the 'mapped_labels' column by converting string representations to lists,\n",
    "    calculate the maximum number of annotators, and expand the 'mapped_labels' into separate columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the 'mapped_labels' column.\n",
    "        exclude_annotator (int): Annotator index to exclude from processing (1-based index).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with expanded annotator columns.\n",
    "    \"\"\"\n",
    " \n",
    "    df['mapped_labels'] = df['mapped_labels'].apply(\n",
    "        lambda value: ast.literal_eval(value) if isinstance(value, str) else value\n",
    "    )\n",
    "    \n",
    "  \n",
    "    max_num_annotators = df['mapped_labels'].apply(lambda x: len(x) if isinstance(x, list) else 0).max()\n",
    "    print(f\"Maximum number of annotators: {max_num_annotators}\")\n",
    "    \n",
    "    \n",
    "    annotator_columns = [f\"answer{i+1}\" for i in range(max_num_annotators)]\n",
    "    \n",
    "  \n",
    "    for i, annotator in enumerate(annotator_columns):\n",
    "    \n",
    "        if exclude_annotator and i + 1 == exclude_annotator:\n",
    "            continue\n",
    "        df[annotator] = df['mapped_labels'].apply(lambda x: x[i] if i < len(x) else 0)\n",
    "    \n",
    "  \n",
    "    if exclude_annotator:\n",
    "        excluded_col = f\"answer{exclude_annotator}\"\n",
    "        if excluded_col in df.columns:\n",
    "            df.drop(columns=[excluded_col], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = process_annotators(train, exclude_annotator=8)  \n",
    "test = process_annotators(test)  \n",
    "val = process_annotators(val)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ = Dataset.from_pandas(train, preserve_index=False)\n",
    "test_ = Dataset.from_pandas(test, preserve_index=False)\n",
    "val_ = Dataset.from_pandas(val, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({'train': train_, 'test': test_, 'val': val_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2100/2100 [00:01<00:00, 1897.09 examples/s]\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 1382.74 examples/s]\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 1566.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_func(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['merged_text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "   \n",
    "    num_annotators = len([col for col in examples if col.startswith('answer')])  \n",
    "    for i in range(1, num_annotators + 1):\n",
    "        annotator_label = f'answer{i}'  \n",
    "        if annotator_label in examples:\n",
    "            tokenized_inputs[annotator_label] = examples[annotator_label]\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized = train_.map(tokenize_func, batched=True)\n",
    "val_tokenized = val_.map(tokenize_func, batched=True)\n",
    "test_tokenized = test_.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_tokenized\n",
    "val_dataset = val_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotator_dataset(dataset, annotator_label):\n",
    "   \n",
    "    if 'label' in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(['label'])\n",
    " \n",
    "    dataset = dataset.rename_column(annotator_label, 'label')\n",
    "    dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_annotator_specific_datasets(tokenized_dataset, annotator_column):\n",
    "    return create_annotator_dataset(tokenized_dataset, annotator_column)\n",
    "\n",
    "\n",
    "max_num_annotators = len([col for col in train_dataset.column_names if col.startswith('answer')])\n",
    "\n",
    "datasets = {}\n",
    "for i in range(1, max_num_annotators + 1):\n",
    "    if i == 8:\n",
    "        print(f\"Skipping answer8 for annotator {i}\")\n",
    "        continue\n",
    "    \n",
    "    annotator_label = f'answer{i}'\n",
    "    datasets[f'train_a{i}'] = create_annotator_specific_datasets(train_tokenized, annotator_label)\n",
    "    datasets[f'val_a{i}'] = create_annotator_specific_datasets(val_tokenized, annotator_label)\n",
    "    datasets[f'test_a{i}'] = create_annotator_specific_datasets(test_tokenized, annotator_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    " \n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy() \n",
    "    cross_entropy = -np.sum(np.eye(probs.shape[1])[labels] * np.log(probs + 1e-9)) / len(labels)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'cross_entropy': cross_entropy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./multiclassification_ensemble_epic{model_name_filename}/results/human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_annotator_model(train_dataset, val_dataset, output_dir):\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "   \n",
    "\n",
    "    best_model_dir = trainer.state.best_model_checkpoint\n",
    "    if best_model_dir:\n",
    "        for checkpoint in os.listdir(output_dir):\n",
    "            checkpoint_path = os.path.join(output_dir, checkpoint)\n",
    "            if checkpoint_path != best_model_dir and os.path.isdir(checkpoint_path):\n",
    "                shutil.rmtree(checkpoint_path)\n",
    "\n",
    "    return trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_all_annotators(max_num_annotators):\n",
    "\n",
    "    models = {}\n",
    "\n",
    "  \n",
    "    for i in range(1, max_num_annotators + 1):\n",
    "      \n",
    "        train_data = datasets[f'train_a{i}']\n",
    "        val_data = datasets[f'val_a{i}']\n",
    "\n",
    "       \n",
    "        output_dir = f\"output_a{i}\"\n",
    "\n",
    "       \n",
    "        model = train_annotator_model(train_data, val_data, output_dir)\n",
    "\n",
    "       \n",
    "        models[f'model_a{i}'] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "max_num_annotators = len([col for col in train_tokenized.column_names if col.startswith('answer')])\n",
    "\n",
    "\n",
    "models = train_for_all_annotators(max_num_annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_confidences_ensemble(df, models, tokenizer, device):\n",
    "   \n",
    "    confidences = []  \n",
    "    softmax_probs = []  \n",
    "\n",
    "  \n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "    for text in df['merged_text']:\n",
    "       \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        ensemble_logits = None\n",
    "      \n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                if ensemble_logits is None:\n",
    "                    ensemble_logits = logits\n",
    "                else:\n",
    "                    ensemble_logits += logits  \n",
    "\n",
    "  \n",
    "        probabilities = torch.nn.functional.softmax(ensemble_logits, dim=-1).cpu().numpy()[0]\n",
    "        confidences.append(probabilities.max())  \n",
    "        softmax_probs.append(probabilities) \n",
    "\n",
    " \n",
    "    df['confidence_scores'] = confidences\n",
    "    df['softmax_probs'] = softmax_probs\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_predictions(model, tokenizer, text, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "    return predicted_class, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def majority_vote(predictions):\n",
    "    \n",
    "    counts = Counter(predictions)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "def evaluate_performance_conf(df, ensemble_models, tokenizer, device):\n",
    "   \n",
    "   \n",
    "   \n",
    "    df['majority_label'] = df['majority_label'].astype(int)  \n",
    "\n",
    " \n",
    "    for model in ensemble_models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    " \n",
    "    majority_preds = []\n",
    "    confidences = []\n",
    "\n",
    "  \n",
    "    for text in df['merged_text']:\n",
    "     \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "        individual_predictions = [] \n",
    "        individual_confidences = []  \n",
    "\n",
    "        for model in ensemble_models:\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "                individual_predictions.append(np.argmax(probabilities))\n",
    "                individual_confidences.append(probabilities.max())\n",
    "\n",
    "       \n",
    "        majority_pred = majority_vote(individual_predictions)\n",
    "        majority_preds.append(majority_pred)\n",
    "\n",
    "        \n",
    "        confidences.append(np.mean(individual_confidences))\n",
    "\n",
    " \n",
    "    df['majority_preds'] = majority_preds\n",
    "    df['confidence_scores'] = confidences\n",
    "\n",
    "  \n",
    "    y_true = df['majority_label']\n",
    "    y_pred = np.array(df['majority_preds'], dtype=int)\n",
    "\n",
    " \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro') \n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro') \n",
    "\n",
    "  \n",
    "    print(\"Accuracy:\", accuracy * 100)\n",
    "    print(\"Precision:\", precision * 100)\n",
    "    print(\"Recall:\", recall * 100)\n",
    "    print(\"F1 Score:\", f1 * 100)\n",
    "\n",
    " \n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    \n",
    "    correct_confidence = np.mean([conf for pred, true, conf in zip(y_pred, y_true, confidences) if pred == true])\n",
    "    incorrect_confidence = np.mean([conf for pred, true, conf in zip(y_pred, y_true, confidences) if pred != true])\n",
    "    avg_confidence = np.mean(confidences)\n",
    "\n",
    "    print(\"Average Confidence Score:\", avg_confidence * 100)\n",
    "    print(\"Average Confidence for Correct Predictions:\", correct_confidence * 100)\n",
    "    print(\"Average Confidence for Incorrect Predictions:\", incorrect_confidence * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
