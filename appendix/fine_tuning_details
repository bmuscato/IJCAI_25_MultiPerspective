
## BERT-large and RoBERTa-large


In this project, we fine-tuned the BERT-large and RoBERTa-large models using both baseline and multi-perspective approaches. For consistency and reproducibility, we relied on standard hyperparameter settings commonly used in the literature. Specifically, each model was trained for a total of **8 epochs** using a **learning rate of 5e-5**, following the approach outlined by Davani et al. (2022). 
To avoid overfitting, we employed **early stopping** with a patience of 3 epochs.

In addition to early stopping, we applied **weight decay** with a coefficient of 0.01 to regularize the training process. We also used **500 warmup steps** to allow the learning rate to increase gradually at the start of training, which can help improve stability and performance. Each training run used a **batch size of 16**.

The fine-tuning process was conducted on a high-performance computing setup comprising **two NVIDIA Tesla V100 GPUs**, each with 32GB of memory. This allowed us to train the models efficiently, even with large input sequences and complex model architectures.

## Loss Function for Baseline Model

For the baseline model, we used a **multi-class cross-entropy loss** function. This loss is well-suited for classification tasks where the objective is to assign an input to one of several classes. It is defined as:

$$
\text{Loss} = - \sum_{i=1}^C y_i \log(p_i)
$$

Here, $C$ represents the total number of classes, $y_i$ is the one-hot encoded true label for class $i$, and $p_i$ is the predicted probability of that class. The loss increases when the model's predicted probabilities deviate from the actual class distribution, thereby encouraging the model to produce accurate predictions over time.


